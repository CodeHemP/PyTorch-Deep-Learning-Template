{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Deep Learning Template\n",
    "### A clean and simple template to kick start your project\n",
    "*Francesco Saverio Zuppichini*\n",
    "\n",
    "This template aims to make it easier for you to start a new deep learning computer vision project with PyTorch. The main features are:\n",
    "\n",
    "- modularity: we splitted each logic piece into a different python submodule\n",
    "- data-augmentation: we included [imgaug](https://imgaug.readthedocs.io/en/latest/)\n",
    "- ready to go: by using [poutyne](https://pypi.org/project/Poutyne/) a Keras like framework you don't have to write any train loop.\n",
    "- reduce learning rate on platue\n",
    "- auto saving the best model\n",
    "- experiment tracking with [comet](https://www.comet.ml/)\n",
    "\n",
    "### Motivation\n",
    "Let's face it, most of data scientist are not software engineers and they usually end up with spaghetti code, most of the times on a big unusable jupyter-notebook. With this repo you have a clean example of how your code should be splitted and modularized in order to make scalability and sharability possible.\n",
    "\n",
    "## Structure\n",
    "```\n",
    ".\n",
    "├── callbacks // here you can create your custom callbacks\n",
    "├── checkpoint // were we store the trained models\n",
    "├── data // here we define our dataset\n",
    "│   ├── MyDataset.py\n",
    "│   └── transformation // custom transformation, e.g. resize and data augmentation\n",
    "├── dataset // the data\n",
    "│   ├── train\n",
    "│   └── val\n",
    "├── logger.py // were we define our logger\n",
    "├── losses // custom losses\n",
    "├── main.py\n",
    "├── models // here we create our models\n",
    "│   ├── MyCNN.py\n",
    "│   ├── resnet.py\n",
    "│   └── utils.py\n",
    "├── playground.ipynb // a notebook that can be used to fast experiment with things\n",
    "├── Project.py // a class that represents the project structure\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── test // you should always perform some basic testing\n",
    "│   └── test_myDataset.py\n",
    "└── utils.py // utilities functions\n",
    "```\n",
    "\n",
    "In this example we will try to classify Darth Vader and Luke Skywalker. We have 100 images per class gathered using google images. The dataset is [here](https://drive.google.com/open?id=1LyHJxUVjOgDIgGJL4MnDhA10xjejWuw7). You just have to exact it in this folder and run main.py. We are finetunig resnet18 and it should be able to reach > 90% accuracy in 5/10 epoches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We strongly encourage to play around with the template**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep your structure clean and concise\n",
    "\n",
    "Every deep learning project has at least three mains steps:\n",
    "\n",
    "- data gathering/processing\n",
    "- modelling\n",
    "- training/evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "One good idea is to store all the paths at interesting location, e.g. the dataset folder, in a shared class that be accessed by anyone in the folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "A wise man once said *every starts with the data*. In the `data` package you can define your own Dataset, as always by subclassing `torch.data.utils.Dataset`. In our example we created a dataset that reads the images in the dataset directory. \n",
    "\n",
    "### Transformation\n",
    "You usually have to do some preprocessing on the data, e.g. resize the images and apply data augmentation. All your transformation should go inside `.data.trasformation`\n",
    "\n",
    "### Dataloader\n",
    "As you know, you have to create a `Dataloader` to feed your data into the model. In this example we have a very simple function `get_dataloaders` in the `data` packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "All your models go inside `models`, in our case we have a very basic cnn and we override the `resnet18` function in order to provide a freezed model to finetune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Evaluation\n",
    "\n",
    "In our case we kept things simple, all the training and evaluation logic is inside `.main.py` where we used [poutyne](https://pypi.org/project/Poutyne/) as main library. We already defined a vailable list of callbacks:\n",
    "\n",
    "- learning rate scheduler\n",
    "- auto save of the best model\n",
    "- early stopping\n",
    "\n",
    "### Track your experiment\n",
    "We are using [comet](https://www.comet.ml/) to automatically track our models results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "We also created different utilities function to plot booth dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
